<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Graduate Student Project</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Graduate Student Project: Teachable Machines</h1>
    </header>
    
    <nav class="floating-nav">
        <a href="#objectives">Project Objectives</a>
        <a href="#scope">Project Scope</a>
        <a href="#process">Process</a>
        <a href="#lessons">Lessons Learned</a>
        <a href="#test-model">Test Model</a>
        <a href="#Github Link">Github Link</a>
    </nav>

    <div class="content">
        <section id="objectives" class="section">
            <h2>Project Objectives</h2>
            <p>In this project, we are officially exposed to machine learning. We will use the machine learning algorithm provided by Teachable Machine to train, and combine it with Buolawmini's book to complete this experimental report and thinking.</p>
        </section>

        <section id="scope" class="section">
            <h2>Project Scope</h2>
            <p>Classification:Hand Posture and Music </p>
            <p>Range: 7 Posture</p>
            <p>Item List:Do, Re, Mi, Fa, So, La, Ti</p>
            <p>Image Samples: over 400 of each hand posture</p>
            
        </section>

        <section id="process" class="section">
            <h2>Process</h2>
            <p>Before the Test</p>
            <p>Because both of us encountered issue using the CAMERAs, one of our group member ordered a BRAND NEW laptop online to use the CAMERA. We really appreciate this course which help us buying a new laptop with super HD CAMERA.</p>
            <!-- <p>TEST 1. Classic Image Classification</p>
            <p>In this test, we will classify some tools, including hammer, screwdriver and plier. We collect the image samples by recording showing the item in front of the web camera. After the first attempt the precision is not that great.
                 As you can see below, the plier was thought to be a screwdriver</p>
            <img src="training1.png" alt="first attempt">
            <p>Then, we collect more sample for those three kinds of tool to improve the precision and add one more class named "hand" to on behalf of our hands.
                And it is proved that it is better than previous one.
            </p>
            <img src="training2.png" alt="second attempt">
            <p>Following is the video recording the test result</p>
            <video src="Test1/TEST1.mp4" controls></video>
            <p>TEST 2. Classic Sound Classification</p>
            <p>The second test is quite similar to the first one. The only difference is the media form. We replaced the image with sound from different musical instruments including guitar, violin and piano.</p>
            <p>One more thing, we have to record the background noise first to decrease the impact of noise on training.</p>
            <img src="training3.png" alt="second test">
            <video src="Test2/TEST2.mp4" controls></video> -->
            <p>TEST: Play Music With Your Hands</p>
            <p>In this test, we decided to play music just with our own hands, I mean, literally. First of all, we trained the model to identify different hand signals(Which is not standard hand singal) and connect them to 1-7. Then we extracted do re mi fa so ra shi so from a piano video as our "key"s. We make them to several single mp4 file. If the model detect certain hand singal, it will play the scale once. We set 1 second common interval to make it play scale every 1 second</p>
            <p>As you may know, playing music in this way require high precision. We tried 100 sample to train each hand singal but it turned out to be not precise. But when we increase the number of sample to 400 each, the problem could be solved 90%. I think it almost reach the limit of this browser-based model because the webpage almost crashed while preparing the data but managed to make it.</P>
            <p>As I mentioned above, it solve the 90% of the problem. It still make mistake so we have to put our hands really close to the camera and try our best to maintain the same hand signal in training set.
                Here is a example clip of litte star we made. We also deployed this in this page, you can try it yourself.
            </p>
            <img src="t1.png" alt="process">
            <video src="Test3/TEST3.mp4" controls></video>

        </section>

        <section id="lessons" class="section">
            <h1>Project Statement</h1>
            <h2>Exploring Machine Learning Applications and Ethical Considerations Inspired by Joy Buolamwini's "Unmasking AI"</h2>

            <p>In our recent group project, we delved into the fundamentals of machine learning by undertaking a series of tasks that included image classification, sound classification, and an advanced challenge of mapping hand gestures to musical notes to perform a song. This project not only allowed us to apply theoretical knowledge in practical scenarios but also prompted us to reflect on the broader implications of artificial intelligence (AI) in society.</p>

            <p>Inspired by Joy Buolamwini's book <em>Unmasking AI: My Mission to Protect What Is Human in a World of Machines</em>, we aimed to understand the ethical considerations surrounding AI, particularly issues of bias, fairness, and inclusivity. Through the lens of our project, we directly engaged with challenges described by Buolamwini—ensuring fairness and accuracy while recognizing hidden biases in datasets.</p>

            <h3>Project Overview</h3>
            <p><strong>1. Image Classification:</strong> We trained a machine learning model to recognize and classify images of basic tools—hammer, pliers, and screwdriver. Using a dataset of labeled images, we employed a convolutional neural network (CNN) to learn distinguishing features and achieve accurate classifications. We started with a small dataset and iteratively expanded it, incorporating diverse samples to increase model robustness. For instance, we captured images under varying lighting conditions and angles, reflecting real-world scenarios.</p>

            <p><strong>2. Sound Classification:</strong> We developed a model to differentiate between distinct sounds: whistling, finger snapping, and clapping. By processing audio samples and extracting features such as frequency and amplitude, the model utilized recurrent neural networks (RNNs) to handle the temporal nature of sound data. This task posed unique challenges, such as ensuring background noise did not interfere with classification accuracy. To address this, we recorded "background noise" as a separate class and used it during training to improve the model's performance.</p>

            <p><strong>3. Hand Gesture Recognition and Musical Mapping:</strong> The most challenging task involved training a model to recognize specific hand gestures and map them to musical notes. By capturing images of various gestures and associating each with a corresponding note, we enabled the model to interpret real-time hand movements and produce a melody. This process required extensive data collection, with hundreds of samples for each gesture, and careful tuning of the model to minimize errors. While the model achieved a high level of accuracy, some limitations remained, especially in distinguishing similar gestures.</p>

            <h3>Methodologies</h3>
            <p><strong>Data Collection and Preprocessing:</strong> For each task, we gathered datasets that were representative and diverse. For image classification, we took photographs of tools under different lighting conditions and orientations. For sound classification, we recorded audio samples in various environments, ensuring that ambient noise was considered. For hand gesture recognition, we captured images of gestures made by individuals with varying hand sizes, skin tones, and accessories.</p>

            <p><strong>Model Training:</strong> We utilized popular machine learning frameworks such as TensorFlow and Keras. For image classification, CNNs were chosen due to their effectiveness in visual pattern recognition. For sound classification, RNNs and Long Short-Term Memory (LSTM) networks were employed to manage sequential data. The hand gesture model leveraged a combination of techniques, including data augmentation, to enhance its ability to generalize.</p>

            <p><strong>Validation and Testing:</strong> We split our datasets into training and testing sets, using techniques like cross-validation to assess model performance and prevent overfitting. This step was critical in identifying areas where the models struggled and required further refinement.</p>

            <h3>Integration of Ethical Considerations</h3>
            <p>Joy Buolamwini's <em>Unmasking AI</em> emphasizes the concept of the "coded gaze," which refers to the biases embedded within AI systems, often reflecting the prejudices of their creators and the data they are trained on. This notion resonated with us and influenced several aspects of our project:</p>

            <p><strong>1. Data Bias Awareness:</strong> Recognizing that biased data leads to biased models, we scrutinized our datasets for diversity. For example, in the hand gesture recognition task, we included images of hands with different skin tones, sizes, and accessories (like rings or bracelets) to prevent the model from underperforming on certain demographics. Similarly, for sound classification, we ensured that recordings were made in both quiet and noisy environments to account for different real-world conditions.</p>

            <p><strong>2. Fairness in Model Performance:</strong> We monitored the model's accuracy across different subsets of data to identify any disparities in performance. For instance, during the image classification task, we found that the model initially struggled to distinguish between pliers and screwdrivers when the lighting was dim. This observation led us to expand our dataset and include more challenging examples, which ultimately improved the model's robustness.</p>

            <p><strong>3. Inclusivity in Design:</strong> We considered the accessibility of our system. By enabling music to be played through gestures, we opened possibilities for individuals who may not have the ability to play traditional instruments. This aspect of our project highlighted the potential for AI to empower users in unique and meaningful ways, aligning with Buolamwini's vision of ethical and inclusive technology.</p>

            <p><strong>4. Reflection on Societal Impact:</strong> Beyond the technical details, our project served as a reminder of the broader implications of AI. The biases and limitations we encountered in our models echoed the challenges faced by real-world systems, particularly in areas like facial recognition and automated decision-making. These reflections reinforced the importance of ethical considerations in every stage of AI development.</p>

            <h3>Challenges and Reflections</h3>
            <p>Despite our best efforts, we encountered several challenges that underscored the complexity of developing fair and accurate AI systems:</p>

            <p><strong>Addressing Unconscious Bias:</strong> Despite our efforts to create diverse datasets, we found that certain biases persisted. For example, the hand gesture model initially struggled with darker skin tones, echoing Buolamwini's experiences with facial recognition technologies. This highlighted the pervasive nature of bias in AI and underscored the importance of continuous vigilance.</p>

            <p><strong>Ethical Data Collection:</strong> We grappled with questions about consent and privacy, particularly when capturing images and sounds that could be personally identifiable. This led us to implement strict protocols for data handling and to anonymize data where possible. While this added complexity to the project, it was a necessary step in ensuring ethical practices.</p>

            <p><strong>Limitations of AI:</strong> Our project reinforced the understanding that AI models are only as good as the data and assumptions they are built upon. This aligns with Buolamwini's assertion that AI reflects both the aspirations and limitations of its creators. The process of refining our models served as a humbling reminder of the challenges inherent in developing fair and effective AI systems.</p>

        </section>

        <section id="test-model" class="section">
            <h2>Test Model</h2>
            <p>Ready to try our model for yourself? Jsut follow our process and have Fun!</p>
            
            <div class="test-model-buttons">
                <!-- <a href="Test1/Test1.html" target="_blank" class="custom-button">Test1</a>
                <a href="Test2/Test2.html" target="_blank" class="custom-button">Test2</a> -->
                <a href="Test3/Test3.html" target="_blank" class="custom-button">Test</a>
            </div>
        </section>

        <section id="Github Link" class="section">
            <h2>Links</h2>
            <div class="test-model-buttons">
                <a href="https://github.com/RemingtonL/Pjct3_TeachableMachine" target="_blank" class="custom-button">Git Hub</a>
                <a href="https://teachablemachine.withgoogle.com/models/NmsqoV1m6/" target="_blank" class="custom-button">Teachable Machine Model</a>
            </div>
        </section>
        
    </div>

    <footer>
        <p>&copy; Rui Xu & Zhihui Lin</p>
    </footer>
</body>
</html>
